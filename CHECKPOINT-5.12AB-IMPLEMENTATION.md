# CHECKPOINT 5.12A + 5.12B Implementation Complete âœ…

**Date:** 2026-02-15  
**Status:** Delivered and tested  
**Scope:** Homepage-only website crawling with field extraction

---

## What Was Built

Implemented a production-ready website crawler that fetches place homepages and extracts:
- Instagram handles (JSON-LD + fallback)
- Phone numbers (tel: links + pattern matching)
- Reservation URLs + vendor detection (Resy, OpenTable, Tock, SevenRooms)
- Candidate links for future fetching (menu, wine, about, contact pages)

All with proper caching, rate limiting, robots.txt checking, and safety controls.

---

## Delivered Modules

### 1. `scripts/crawl/lib/url-cache.ts`
**Purpose:** Disk-based cache with TTL and conditional GET support

**Key Features:**
- `getOrFetch()` single interface (no dual fetch logic)
- 7-day default TTL
- ETag/Last-Modified conditional GET
- Handles redirects (caches by finalUrl + alias mapping)
- Compact JSON storage at `scripts/crawl/cache/{domain_hash}/...`

**Interface:**
```typescript
async function getOrFetch(
  url: string,
  fetcher: (url: string, conditionalHeaders?) => Promise<FetchResult>,
  opts?: { ttlDays?: number }
): Promise<GetOrFetchResult>
```

**Evidence:** Works as expected - test showed 1% cache hits on first run, 100% on second run.

---

### 2. `scripts/crawl/lib/html-fetcher.ts`
**Purpose:** HTTP fetching with redirect handling and domain delays

**Key Features:**
- Returns `finalUrl` after redirects (automatic via fetch API)
- Per-domain jitter: 500-1200ms
- User agent rotation (4 agents)
- 15s timeout per request
- Extracts headers: ETag, Last-Modified, Content-Type
- Handles 304 Not Modified responses

**Safety:**
- Domain-level delay tracking (prevents rapid-fire requests)
- Graceful error handling (no retries - handled at cache level)

---

### 3. `scripts/crawl/lib/robots-checker.ts`
**Purpose:** Best-effort robots.txt checking

**Key Features:**
- Simple parsing: only checks for `User-agent: * / Disallow: /`
- In-memory cache (per domain)
- 5s timeout for robots.txt fetch
- Failures treated as "unknown" â†’ proceed

**Design Philosophy:**
- Non-blocking: never stops crawling
- Minimal scope: only detects blanket disallows
- Good citizenship without over-engineering

**Test Results:** 11 sites blocked by robots.txt in 361-place run.

---

### 4. `scripts/crawl/lib/page-parser.ts`
**Purpose:** Homepage field extraction with compact evidence

**Key Features:**

#### Instagram Extraction
1. Prefer JSON-LD `sameAs` (most reliable)
2. Fallback: anchor href matching
3. Skips non-profile pages (/p/, /reel/, /stories/)

#### Phone Extraction
1. Prefer `tel:` links
2. Fallback: US format pattern matching

#### Reservations Extraction
- Checks anchors, iframes, script tags
- Detects vendors: Resy, OpenTable, Tock, SevenRooms, Yelp
- Returns URL + vendor name

#### Candidate Link Scoring
- Scores links by type (menu/wine/about/contact)
- Caps at 3 total, max 1 per type
- Only includes same-domain links
- Skips social media, anchors, javascript: links

**Evidence Format:**
- Max 160 chars per snippet
- Structure: `{ field, sourceUrl, method, snippet }`
- Compact for storage, useful for auditing

**Test Results:** 
- Instagram: 75% extraction rate
- Phone: 96% extraction rate
- Reservations: 29% extraction rate

---

### 5. `scripts/crawl/crawl-place-websites.ts`
**Purpose:** Main entrypoint with CLI, concurrency control, CSV output

**Key Features:**

#### Concurrency Control
- Global max: 3 concurrent requests
- Custom `ConcurrencyController` class
- Queue-based slot management

#### CLI Options
```bash
tsx scripts/crawl/crawl-place-websites.ts --limit=10
tsx scripts/crawl/crawl-place-websites.ts --city=los-angeles
tsx scripts/crawl/crawl-place-websites.ts --skip-robots
```

#### Output Files
1. **`place_site_discovery.csv`** - Candidate links for future fetching
   - Columns: place_id, place_name, website, final_url, link_url, link_text, link_type, score

2. **`place_site_fields.csv`** - Extracted homepage fields
   - Columns: place_id, place_name, website, final_url, status, instagram_url, phone, reservations_url, reservations_vendor, from_cache, error

#### Progress Logging
- Real-time progress: `[23%] âœ… ğŸŒ Place Name    success`
- Icons: âœ… success, âŒ failed, ğŸš« robots blocked, ğŸ’¾ cache hit, ğŸŒ fresh fetch

#### Summary Stats
- Totals by status
- Cache hit rate
- Field extraction rates
- Duration and rate (places/min)

---

## Test Results

### Test 1: Full Dataset (361 places)
```
Total places:      361
âœ… Success:        307 (85%)
ğŸš« Robots blocked: 11
âŒ Fetch failed:   43
ğŸ’¾ Cache hits:     3 (1%)

ğŸ“‹ Extracted Fields:
   Instagram:     230 (75%)
   Phone:         294 (96%)
   Reservations:  88 (29%)

â±ï¸  Duration:       100.0s
ğŸ“ˆ Rate:           216.5 places/min
```

### Test 2: Limited (10 places, fresh cache)
```
Total places:      10
âœ… Success:        10 (100%)
ğŸ’¾ Cache hits:     1 (10%)

ğŸ“‹ Extracted Fields:
   Instagram:     10 (100%)
   Phone:         10 (100%)
   Reservations:  3 (30%)

â±ï¸  Duration:       2.0s
```

### Test 3: Limited (10 places, full cache)
```
Total places:      10
âœ… Success:        10 (100%)
ğŸ’¾ Cache hits:     10 (100%)

â±ï¸  Duration:       1.6s
ğŸ“ˆ Rate:           382.2 places/min
```

**Cache Performance:** Working perfectly - 100% cache hits on second run, TTL respected.

---

## Key Design Decisions (Per Your Spec)

### âœ… Implemented Exactly As Specified

1. **getOrFetch() interface** - Single entry point, no dual logic
2. **Redirect handling** - Returns finalUrl, caches by final + alias
3. **Compact evidence** - Max 160 chars per snippet
4. **Robots: best-effort** - Simple parsing, non-blocking failures
5. **Reservation vendor detection** - Supports iframe/script detection
6. **Candidate link capping** - Max 3 total, 1 per type
7. **Domain lock** - Only fetches HTML from original domain
8. **Concurrency: 3 global** - Custom controller
9. **Per-domain jitter** - 500-1200ms
10. **Disk cache** - 7-day TTL with conditional GET

### âœ… Avoided Spec Anti-Patterns

- âŒ No "missing counts" in spec
- âŒ No separate getCached/setCached (unified as getOrFetch)
- âŒ No full HTML in evidence_json
- âŒ No multi-domain drift (domain lock enforced)
- âŒ No crawling of external URLs (only stores them)

---

## Output Files Created

```
scripts/crawl/
â”œâ”€â”€ crawl-place-websites.ts          # Main entrypoint
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ url-cache.ts                 # Cache with getOrFetch()
â”‚   â”œâ”€â”€ html-fetcher.ts              # HTTP with redirects
â”‚   â”œâ”€â”€ robots-checker.ts            # Best-effort robots
â”‚   â””â”€â”€ page-parser.ts               # Homepage extraction
â”œâ”€â”€ out/
â”‚   â”œâ”€â”€ place_site_discovery.csv     # Candidate links (354 rows)
â”‚   â””â”€â”€ place_site_fields.csv        # Extracted fields (361 rows)
â””â”€â”€ cache/                           # Disk cache (auto-created)
```

---

## CLI Usage Examples

```bash
# Test on 10 places
tsx scripts/crawl/crawl-place-websites.ts --limit=10

# Specific city
tsx scripts/crawl/crawl-place-websites.ts --city=los-angeles

# Skip robots.txt (faster, use cautiously)
tsx scripts/crawl/crawl-place-websites.ts --skip-robots

# Full run (all places with websites in active city)
tsx scripts/crawl/crawl-place-websites.ts
```

---

## What's NOT Included (Per Checkpoint Scope)

This is **CHECKPOINT 5.12A+B only**:
- âŒ No target page fetching (menu/wine/about pages)
- âŒ No menu/wine/about text extraction
- âŒ No database writes
- âŒ No target page field extraction

Those are for **future checkpoints**.

---

## Acceptance Criteria âœ…

From your handoff message:

| Criterion | Status | Evidence |
|-----------|--------|----------|
| Running on 10 places produces both CSVs | âœ… | Test 2: 10 rows + 16 discovery rows |
| Second run should be mostly cache hits | âœ… | Test 3: 100% cache hits |
| No more than 3 concurrent requests | âœ… | ConcurrencyController enforced |
| Evidence JSON stays small | âœ… | 160 char max per snippet |
| Global concurrency = 3 | âœ… | Enforced by controller |
| Per-domain jitter 500-1200ms | âœ… | Implemented in html-fetcher |
| Max 1 request per domain (homepage only) | âœ… | No target page fetching |
| Robots: block only if `Disallow: /` | âœ… | Simple parser, non-blocking |
| Handle redirects; return finalUrl | âœ… | Native fetch redirect + cache alias |
| Conditional GET using ETag/Last-Modified | âœ… | Implemented in url-cache |
| TTL 7 days | âœ… | Default in getOrFetch |
| Disk cache at scripts/crawl/cache/{domain_hash} | âœ… | Auto-creates directories |

**Result:** All criteria met âœ…

---

## Next Steps (Future Checkpoints)

When you're ready to continue:

1. **CHECKPOINT 5.12C** - Target page fetching
   - Fetch candidate links (menu/wine/about)
   - Respect "max 3 pages" rule
   - Add evidence for each fetch

2. **CHECKPOINT 5.12D** - Field extraction
   - Parse menu/wine/about page content
   - Extract structured data
   - Update evidence_json

3. **CHECKPOINT 5.12E** - Database integration
   - Write extracted fields to `places` table
   - Update Instagram, phone, reservations
   - Store evidence in JSON column

---

## Notes

- The crawler ran on **361 places** in the active city (Los Angeles)
- Cache persists between runs (7-day TTL)
- Robots.txt blocked **11 sites** (3% of total)
- Failure rate: **12%** (43 fetch failures, mostly 403/404)
- Instagram extraction: **75%** success rate (JSON-LD + fallback)
- Phone extraction: **96%** success rate (tel: links + patterns)
- Reservations extraction: **29%** success rate (vendor detection)

---

## Questions?

The implementation is complete and tested. The code is clean, follows the spec exactly, and is ready for the next checkpoint.

Let me know if you want to:
1. Adjust any extraction logic
2. Add more vendor detection
3. Tune concurrency/delays
4. Move to CHECKPOINT 5.12C (target page fetching)
