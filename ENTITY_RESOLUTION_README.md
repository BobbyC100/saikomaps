# Saiko Maps - Entity Resolution System

## Overview

This entity resolution system implements Master Data Management (MDM) patterns to handle multi-source data integration. It follows industry-standard practices from Foursquare, Yelp, and SafeGraph.

**Core Principle:** Never overwrite data. Store every claim from every source, then compute a "Golden Record" view using survivorship rules.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    SAIKO RESOLVER PIPELINE                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  raw_records        entity_links       golden_records           │
│  (all claims)   →   (mappings)     →   (the "truth")           │
│                          │                                       │
│                          ↓                                       │
│                   review_queue                                   │
│                (human review for                                 │
│                 ambiguous matches)                               │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Database Schema

### 1. `raw_records` - The Ingestion Layer
Every incoming POI from any source lands here exactly as received.

- **Never modified or deleted** - append-only log
- Stores full raw JSON blob from source
- Extracted fields for indexing (name_normalized, lat, lng)
- H3 spatial index for blocking
- Placekey support for exact matching

### 2. `entity_links` - The Resolver Layer
Maps raw records to canonical entities.

- Links raw_id → canonical_id
- Stores match confidence and method
- Tracks match features for debugging

### 3. `golden_records` - The Presentation Layer
The computed "truth" that the Saiko app queries.

- Generated by survivorship service
- Each field won by highest-priority source
- source_attribution tracks which source won each field
- Updated automatically when entity_links change

### 4. `review_queue` - Human-in-the-Loop
Catches matches that need manual verification.

- Confidence between 0.70-0.90 requires review
- 2-second decision loop UI
- Tracks resolution history in audit log

## Getting Started

### 1. Export Existing Places

First, seed the entity resolution system with your existing places:

```bash
# Dry run to see what would happen
npm run export:resolver:dry

# Actually export (creates raw_records, entity_links, golden_records)
npm run export:resolver
```

This will:
- Create a `raw_record` for each existing place (source: `saiko_seed`)
- Create an `entity_link` mapping to a new canonical ID
- Generate `golden_records` using survivorship rules

### 2. Ingest Editorial Data

Ingest CSV data from editorial sources:

```bash
npm run ingest:csv data/eater-la.csv editorial_eater
```

Expected CSV format:
```csv
Name,Neighborhood,Category,Address,Source,SourceURL,Phone,Website,Instagram
The Bird,Echo Park,American,1355 W Sunset Blvd,Eater LA,https://...,,,
```

This creates `raw_records` with `is_processed: false` for the resolver to handle.

### 3. Run Resolver Pipeline

The resolver finds matches and creates entity links:

```bash
# Dry run to see what would happen
npm run resolver:dry

# Actually run resolver
npm run resolver:run
```

The resolver uses:
- **Phase 1: Placekey pre-pass** - Auto-link exact placekey matches (confidence: 1.0)
- **Phase 2: H3 blocking** - Only compare records within same or neighboring hexagons
- **Feature comparison** - Name similarity (Jaro-Winkler), distance, address

Match outcomes:
- **Confidence ≥ 0.90** → Auto-link
- **0.70 ≤ Confidence < 0.90** → Send to review queue
- **Confidence < 0.70** → Keep separate

### 4. Review Ambiguous Matches

For matches that need human review, visit the Review Queue UI:

```
http://localhost:3000/admin/review
```

Keyboard shortcuts:
- `M` - Same Place (merge)
- `D` - Different (keep separate)
- `S` - Skip (review later)
- `F` - Flag for senior review
- `←` `→` - Navigate queue

## Survivorship Rules

When multiple sources provide data for the same field, these rules determine which value wins:

| Field | Priority (highest → lowest) | Rationale |
|-------|----------------------------|-----------|
| **lat/lng** | Google > Foursquare > Editorial | Google's "Storefront" geocoding is most precise |
| **name** | Editorial > Google > Foursquare | Editorial may have preferred local name |
| **hours** | Google > Foursquare > Editorial | Google has highest refresh rate |
| **phone** | Google > Editorial > Foursquare | Google verifies via call data |
| **website** | Editorial > Google > Foursquare | Editorial often has direct/preferred URL |
| **instagram** | Editorial > SaikoAI > (never Google) | Editorial has curated handles |
| **description** | Editorial > SaikoAI > Google | Editorial voice is the product |
| **vibe_tags** | Editorial > SaikoAI > (never Google) | Saiko's USP |
| **category** | Editorial > Google > Foursquare | Editorial knows Saiko taxonomy |

## API Routes

### GET `/api/admin/review-queue`
List pending review items.

Query params:
- `status` - 'pending', 'resolved', 'deferred', 'flagged'
- `conflict_type` - Filter by conflict type
- `limit` - Number of items (default: 20)
- `offset` - Pagination offset

### POST `/api/admin/review-queue/:id/resolve`
Resolve a review queue item.

Body:
```json
{
  "resolution": "merged" | "kept_separate" | "flagged",
  "resolution_notes": "Optional explanation",
  "canonical_id": "Optional existing canonical to merge into"
}
```

### POST `/api/admin/review-queue/:id/skip`
Skip/defer a review queue item.

Body:
```json
{
  "reason": "Optional reason",
  "decrease_priority": true
}
```

## Scripts Reference

| Command | Description |
|---------|-------------|
| `npm run export:resolver` | Export existing places to raw_records |
| `npm run export:resolver:dry` | Dry run of export |
| `npm run resolver:run` | Run resolver pipeline |
| `npm run resolver:dry` | Dry run of resolver |
| `npm run ingest:csv <path> <source>` | Ingest editorial CSV |

## Data Flow Example

### Scenario: Ingesting Eater LA data

1. **Ingest CSV**
   ```bash
   npm run ingest:csv data/eater-la.csv editorial_eater
   ```
   - Creates 100 `raw_records` with source: `editorial_eater`
   - All marked `is_processed: false`

2. **Run Resolver**
   ```bash
   npm run resolver:run
   ```
   - Placekey pre-pass: 0 auto-linked (editorial doesn't have placekeys)
   - H3 blocking: Finds candidates within ~100m
   - Feature comparison: Scores each candidate
   - Results:
     - 60 auto-linked (confidence ≥ 0.90)
     - 25 sent to review (0.70 ≤ confidence < 0.90)
     - 15 kept separate (confidence < 0.70)

3. **Review Queue**
   - Visit `http://localhost:3000/admin/review`
   - Review 25 ambiguous matches
   - Press `M` for same place, `D` for different

4. **Golden Records Updated**
   - After each resolution, survivorship runs automatically
   - Editorial wins for: name, website, instagram, description, vibe_tags
   - Google wins for: lat/lng, hours, phone, price_level

## Troubleshooting

### Issue: "No candidates found" for all records
**Cause:** Missing H3 indexes or coordinates  
**Fix:** Ensure lat/lng are present. For editorial data without coordinates, implement geocoding in `ingest-editorial-csv.ts`

### Issue: Too many items in review queue
**Cause:** AUTO_LINK_THRESHOLD too high  
**Fix:** Lower threshold in `scripts/resolver-pipeline.ts` (try 0.85 instead of 0.90)

### Issue: Golden record not updating
**Cause:** Survivorship service error  
**Fix:** Check logs, run manually:
```typescript
import { updateGoldenRecord } from '@/lib/survivorship';
await updateGoldenRecord('canonical_id');
```

### Issue: Slug collisions
**Cause:** Multiple places with same name + neighborhood  
**Fix:** System auto-increments (e.g., `place-2`, `place-3`). This is expected behavior.

## Next Steps

### Phase 1: Backfill Data (Recommended)
1. ✅ Export existing places
2. ✅ Ingest editorial CSVs
3. ⏳ Implement Instagram scraper (parse websites for handles)
4. ⏳ Implement quote extraction background job
5. ⏳ Implement Google Places backfill for missing phone/website

### Phase 2: AI Content Generation
1. ⏳ Generate vibe tags from reviews/descriptions
2. ⏳ Detect restaurant groups via pattern matching
3. ⏳ Generate curator notes (AI drafts, human approval)

### Phase 3: Refresh Pipeline
1. ⏳ Schedule daily hours updates from Google Places
2. ⏳ Schedule weekly photo refreshes
3. ⏳ Monitor for new editorial coverage

## File Structure

```
├── prisma/
│   └── schema.prisma          # Database schema with MDM tables
├── lib/
│   ├── haversine.ts           # Distance calculations
│   ├── similarity.ts          # String matching algorithms
│   ├── survivorship.ts        # Golden record computation
│   └── review-queue.ts        # Review queue business logic
├── scripts/
│   ├── export-to-resolver.ts  # Export existing places
│   ├── resolver-pipeline.ts   # Entity resolution engine
│   └── ingest-editorial-csv.ts # CSV ingestion
├── app/
│   ├── api/admin/review-queue/ # Review queue API routes
│   └── admin/review/          # Review Queue UI
│       ├── page.tsx
│       └── components/
│           ├── ReviewQueue.tsx
│           ├── ComparisonCard.tsx
│           ├── FieldRow.tsx
│           ├── ActionBar.tsx
│           ├── ProximityBadge.tsx
│           └── QueueHeader.tsx
```

## Additional Resources

- [Specification Docs](./DATA_PIPELINE_SESSION_STARTER.md) - Full technical details
- [H3 Documentation](https://h3geo.org/) - Spatial indexing
- [Placekey.io](https://www.placekey.io/) - Universal POI identifiers
- [Dedupe Library](https://github.com/dedupeio/dedupe) - ML deduplication (optional)
